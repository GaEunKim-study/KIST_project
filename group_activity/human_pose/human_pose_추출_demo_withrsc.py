# -*- coding: utf-8 -*-
"""human_pose_추출_demo_withRSC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZlKT396pCrBmlkxEtE89K-e1yDTh1GDZ
"""

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import argparse
import imageio

import scipy.misc
from models import hmr, SMPL
import config
import constants
import torch
from torchvision.transforms import Normalize
import numpy as np
from utils.renderer import Renderer
from PIL import Image
import pickle

'''
parser = argparse.ArgumentParser()
parser.add_argument('--checkpoint', required=True, type=str,
                    help='Path to network checkpoint')
parser.add_argument('--img_path', required=True,
                    type=str, help='Testing image path')
'''

def size_to_scale(size):
    if size >= 224:
        scale = 0
    elif 128 <= size < 224:
        scale = 1
    elif 64 <= size < 128:
        scale = 2
    elif 40 <= size < 64:
        scale = 3
    else:
        scale = 4
    return scale


def get_render_results(vertices, cam_t, renderer):
    rendered_people_view_1 = renderer.visualize(
        vertices, cam_t, torch.ones((images.size(0), 3, 224, 224)).long() * 255)
        #, angle=45, rot_axis=[0, 1, 0])
    return rendered_people_view_1

if __name__ == '__main__':
    '''
    args = parser.parse_args()
    img_path = args.img_path
    checkpoint_path = args.checkpoint
    '''
    img_folder = './composer/test_folder'
    checkpoint_path = './pretrained/RSC-Net.pt'
    
    normalize_img = Normalize(
        mean=constants.IMG_NORM_MEAN, std=constants.IMG_NORM_STD)
    device = torch.device(
        'cuda') if torch.cuda.is_available() else torch.device('cpu')

    hmr_model = hmr(config.SMPL_MEAN_PARAMS)
    checkpoint = torch.load(checkpoint_path)
    hmr_model.load_state_dict(checkpoint, strict=False)
    hmr_model.eval()
    hmr_model.to(device)

    smpl_neutral = SMPL(config.SMPL_MODEL_DIR, create_transl=False).to(device)
    img_renderer = Renderer(focal_length=constants.FOCAL_LENGTH,
                            img_res=constants.IMG_RES, faces=smpl_neutral.faces)
    '''
    temp_face = img_renderer.faces
    print(temp_face)
    print(temp_face.shape)   # (13776, 3)
    '''
    #/crop_img7.jpg
    
    joints_per_person = []
    for i in range(0,1):
        img_path = img_folder +'/test.png'
        img = imageio.v2.imread(img_path)

        im_size = img.shape[0]
        im_scale = size_to_scale(im_size)
        #img_up = scipy.misc.imresize(img, [224, 224])
        img_up = np.array(Image.fromarray(img).resize([224, 224]))
        img_up = np.transpose(img_up.astype('float32'), (2, 0, 1)) / 255.0
        img_up = normalize_img(torch.from_numpy(img_up).float())
        images = img_up[None].to(device)

        with torch.no_grad():
            pred_rotmat, pred_betas, pred_camera, _ = hmr_model(
                images, scale=im_scale)
            pred_output = smpl_neutral(betas=pred_betas, body_pose=pred_rotmat[:, 1:],
                                    global_orient=pred_rotmat[:, 0].unsqueeze(1), pose2rot=False)
            pred_vertices = pred_output.vertices
            pred_joints = pred_output.joints
            pred_cam_t = torch.stack([pred_camera[:, 1],
                                    pred_camera[:, 2],
                                    2 * constants.FOCAL_LENGTH / (constants.IMG_RES * pred_camera[:, 0] + 1e-9)],
                                    dim=-1)
        

        import numpy as np
        import matplotlib.pyplot as plt
        keypoints_3d = []
        
        for i in range(0, 49):
            keypoints_3d.append(pred_joints[:, i, :].tolist())
        
        # 0이 얼굴의 중심(코), 1이 목
        # 0-7 (척추 부분?)
        # 1-5 (목 - 오른쪽 어깨)
        # 1-2 (목 - 왼쪽 어깨)
        # 5-6-7 (오른 팔)
        # 2-3-4 (왼쪽 팔)
        # 8이 꼬리뼈 (댜리 뻗어나오는 곳)
        # 8-13 -14(오른 다리)
        # 8-10 -11 (왼쪽 다리)
        #### 9왼, 12오 - hips
        # 15 - 16이 눈의 keypoints
        # 17 - 18이 귀의 keypoints
        # 19 - 20이 왼발
        # 20 - 21 ?? 뒷꿈치?
        I = np.array([ 1,  5, 6, 1, 1,  8,  10, 8,  13, 2, 3, 1,  1 , 15])  # start points
        J = np.array([ 5,  6, 7, 2, 8,  10, 11, 13, 14, 3, 4, 15, 16, 16])  # end points

        fig = plt.figure(figsize=(20, 20))
        image_ax = fig.add_subplot(1, 2, 1)
        image_ax.imshow(img)
        ax = fig.add_subplot(1,2,2, projection='3d')
        RADIUS = 1 # space around the subject
        xroot, yroot, zroot = keypoints_3d[0][0][0], keypoints_3d[0][0][1], keypoints_3d[0][0][2]
        
        ax.set_xlim3d([-RADIUS+xroot, RADIUS+xroot])
        ax.set_ylim3d([-RADIUS+zroot, RADIUS+zroot])
        #ax.set_zlim3d([-RADIUS-yroot, RADIUS-yroot])
        
        for i in range(0, 17):
            ax.scatter(keypoints_3d[i][0][0], keypoints_3d[i][0][2], -keypoints_3d[i][0][1])
        
        for i in range(len(I)):
            #ax.scatter(keypoints_3d[i][0][0], keypoints_3d[i][0][1], keypoints_3d[i][0][2])
            x, y, z = [np.array( [keypoints_3d[I[i]][0][j], keypoints_3d[J[i]][0][j]] ) for j in range(3)]
            ax.plot(x, z, -y, lw=2, color = (1, 0, 0))
        #ax.plot(x, z, -y, lw=2, color = (1, 0, 0))

        plt.show()


        view_1 = get_render_results(pred_vertices, pred_cam_t, img_renderer)
        view_1 = view_1[0].permute(1, 2, 0).numpy()

        tmp = img_path.split('.')
        name_1 = '.'.join(tmp[:-2] + [tmp[-2] + '_view1'] + ['jpg'])

        imageio.imwrite(name_1, (view_1 * 255).astype(np.uint8))

        KEYPOINT_INDEXES = {
        'nose': 0,
        'left_eye': 1,
        'right_eye': 2,
        'left_ear': 3,
        'right_ear': 4,
        'left_shoulder': 5,
        'right_shoulder': 6,
        'left_elbow': 7,
        'right_elbow': 8,
        'left_wrist': 9,
        'right_wrist': 10,
        'left_hip': 11,
        'right_hip': 12,
        'left_knee': 13,
        'right_knee': 14,
        'left_ankle': 15,
        'right_ankle': 16
        }
        
        KEYPOINT_INDEXES_RSCNET = {
        'nose': 0,
        'left_eye': 15,
        'right_eye': 16,
        'left_ear': 17,
        'right_ear': 18,
        'left_shoulder': 2,
        'right_shoulder': 5,
        'left_elbow': 3,
        'right_elbow': 6,
        'left_wrist': 4,
        'right_wrist': 7,
        'left_hip': 9,
        'right_hip': 12,
        'left_knee': 10,
        'right_knee': 13,
        'left_ankle': 11,
        'right_ankle': 14,
        }
        '''
        'neck' : 1,
        'coccyx': 8
        '''
    
        #python demo.py --checkpoint=./pretrained/RSC-Net.pt --img_path=./composer/crop_img/crop_img7.jpg
        data = {}
        for i in range(0, 19):
            x, z, y = keypoints_3d[i][0][0], keypoints_3d[i][0][2], keypoints_3d[i][0][1]
            data[str(i)] = [x,z,-y]
        mapping_data = {}
        idx = 0
        for key, value in KEYPOINT_INDEXES_RSCNET.items():
            for key_data, value_data in data.items():
                if str(value) == key_data:
                    mapping_data[idx] = value_data
                    idx += 1
                    break
        joints = []
        for key, value in mapping_data.items():
            x, z, y = value
            joints.append([x, z, y, key])
        joints_per_person.append(list(joints))
    #[x, z, -y, keypoints index] 형식의 pickle file 생성
    with open('person_joint.pickle', 'wb') as file:  
        pickle.dump(joints_per_person, file)