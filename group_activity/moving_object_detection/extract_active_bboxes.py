# -*- coding: utf-8 -*-
"""extract_active_bboxes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nFovtmgSa5boVjqZd53SDjG5UjG0VPUj
"""

# Copyright (c) Facebook, Inc. and its affiliates.
import argparse
import multiprocessing as mp
import numpy as np
import os
import shutil
import cv2
import tqdm
import torch
import graph
import tools
import networks
from torchvision import transforms
from datasets import extract_feat_info
import pickle
import traceback
import time

from detectron2.config import get_cfg
from detectron2.data.detection_utils import read_image
from detectron2_predictor import VisualizationDemo

# constants
WINDOW_NAME = "COCO detections"

def ErrorLog(error: str):
    current_time = time.strftime("%Y.%m.%d/%H:%M:%S", time.localtime(time.time()))
    with open("Log.txt", "a") as f:
        f.write(f"[{current_time}] - {error}\n")

def create_folder(path):
    try:
        if not os.path.exists(path): os.makedirs(path)
    except OSError:
        print('Error: Creating directory.', path)

def load_data(args):
    with open(file=os.path.join(args.output, 'active_bboxes.pickle'), mode="rb") as fr:
        loaded_data = pickle.load(fr)
        return loaded_data

def save_data(args, data):
    with open(file=os.path.join(args.output, 'active_bboxes.pickle'), mode='wb') as fw:
        pickle.dump(data, fw, protocol=pickle.HIGHEST_PROTOCOL)

def setup_cfg(args):
    # load config from file and command-line arguments
    cfg = get_cfg()
    # To use demo for Panoptic-DeepLab, please uncomment the following two lines.
    # from detectron2.projects.panoptic_deeplab import add_panoptic_deeplab_config  # noqa
    # add_panoptic_deeplab_config(cfg)
    cfg.merge_from_file(args.config_file)
    cfg.merge_from_list(args.opts)
    cfg.SOLVER.IMS_PER_BATCH = 2
    # Set score_threshold for builtin models
    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = args.confidence_threshold
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = args.confidence_threshold
    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = args.confidence_threshold
    cfg.freeze()
    return cfg

def get_parser():
    parser = argparse.ArgumentParser(description="combine Detectron2 with Tokencut Video.")

    ###### detectron2 ######
    parser.add_argument(
        "--config-file",
        default="./detectron2/configs/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml",
        metavar="FILE",
        help="path to config file",
    )
    parser.add_argument("--data-path", help="Path to root data path")
    parser.add_argument("--output", help="output directory")
    parser.add_argument("--visualize", action="store_true", help="Visualize active people's bboxes, tokencut's segmentation, detectron2's person object detection")

    parser.add_argument("--confidence-threshold", type=float, default=0.5, help="Minimum score for instance predictions to be shown")
    parser.add_argument("--opts", help="Modify config options using the command-line 'KEY VALUE' pairs", default=["MODEL.WEIGHTS", "detectron2://COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/model_final_68b088.pkl"], nargs=argparse.REMAINDER)

    ###### tokencut ######
    parser.add_argument("--arch", default="vit_small", type=str, choices=["vit_small", "vit_base", "moco_vit_small", "mae_vit_base"], help="Model architecture.")
    parser.add_argument("--patch-size", default=16, type=int, help="Patch resolution of the model.")
    parser.add_argument("--min-size", type=int, default=320, help="minimum resolution of image size")
    parser.add_argument("--tau", type=float, default=0.3, help="hyper-parameter used in similarities")
    parser.add_argument("--gap", type=int, default=1, help="frame gap between flow")
    parser.add_argument("--fusion-mode", type=str, default='mean', choices=['mean', 'max', 'min', 'img', 'flow'], help="frame gap between flow")
    parser.add_argument("--flow-model", type=str, default='RAFT', help="frame gap between flow")
    parser.add_argument("--alpha", type=float, default=0.5, help="hyper-parameter used in similarities")
    parser.add_argument("--max-frame", type=int, default=90, help="Max number of frame when building graph")

    ## --- parameters used in bilateral solver
    parser.add_argument("--bs", action="store_true", help="Using bilateral solver")
    parser.add_argument('--sigma-spatial', type=float, default=16, help='sigma spatial in the bilateral solver')
    parser.add_argument('--sigma-luma', type=float, default=16, help='sigma luma in the bilateral solver')
    parser.add_argument('--sigma-chroma', type=float, default=8, help='sigma chroma in the bilateral solver')

    ## --- parameters used in crf
    parser.add_argument("--crf", action="store_true", help="Using crf")
    parser.set_defaults(crf=True)

    return parser

def get_tokencut_mask(args, data_path):

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

    # load models + datasets
    model = networks.get_model(args.arch, args.patch_size, device)
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),])
    video, clip = data_path.split('/')[-2:]
    shutil.copytree(data_path, './raft/input', dirs_exist_ok=True) # data_path 폴더에 있는 파일들을 ./raft/input에 복사
    os.chdir(os.path.abspath("./raft")) ## cannot do it in the loop, as we cannot cd raft twice...

    # flow 데이터 생성
    flow_model = './raft-things.pth'  # model
    flow_path = './RAFT_Flows_gap1'   # where to raw flow
    flow_img_path = './RAFT_FlowImages_gap1'   # where to save the image flow
    cmd = f"python predict.py  --gap 1 --model {flow_model} --path './input' --outroot {flow_img_path} --reverse 0 --raw_outroot {flow_path} --resize {args.min_size} "
    os.system(cmd)

    os.chdir(os.path.abspath(".."))
    shutil.rmtree('./raft/input')
    temp_dir = "./temp"
    create_folder(temp_dir)

    shutil.copytree(os.path.join('./raft', f'RAFT_Flows_gap1/input'), os.path.join(temp_dir, 'RAFT_Flows_gap1'), dirs_exist_ok=True)
    shutil.copytree(os.path.join('./raft', f'RAFT_FlowImages_gap1/input'), os.path.join(temp_dir, 'RAFT_FlowImages_gap1'), dirs_exist_ok=True)
    shutil.rmtree(os.path.join('./raft', f'RAFT_Flows_gap1'))
    shutil.rmtree(os.path.join('./raft', f'RAFT_FlowImages_gap1'))

    flow_img_dir = os.path.join(temp_dir, 'RAFT_FlowImages_gap1')
    flow_dir = os.path.join(temp_dir, 'RAFT_Flows_gap1')

    # extract features from frame
    img_names, nb_node, nb_img, feat_h, feat_w, feats, arr_h, arr_w, frame_id, pil, _ = \
        extract_feat_info('', data_path, args.patch_size, args.min_size, args.arch, model, transform)
    # extract features from flow
    img_names, _, nb_flow, feat_h_flow, feat_w_flow, feats_flow, arr_h_flow, arr_w_flow, frame_id, _, flow = \
        extract_feat_info('', data_path, args.patch_size, args.min_size, args.arch, model, transform, flow_img_dir, flow_dir)

    assert nb_flow == nb_img 
    assert feat_h == feat_h_flow
    assert feat_w == feat_w_flow
    shutil.rmtree(os.path.join('./temp'))

    # Building the graph
    print(f"Building the graph of Tokencut [VIDEO # {video}, CLIP # {clip}]")
    foreground = graph.build_graph(nb_img, nb_node, feats, feats_flow, frame_id, arr_w, arr_h, args.tau, args.alpha, fusion_mode = args.fusion_mode, max_frame=args.max_frame)
    foreground = foreground.reshape(nb_img, feat_h, feat_w)

    if args.visualize:
        tokencut_output_dir = os.path.join(args.output, "tokencut_segmentation", video, clip)
        create_folder(tokencut_output_dir)

    # Generating masks for input video
    masks = []
    for img_id in tqdm.trange(nb_img, desc=f"Tokencut Masks [VIDEO # {video}, CLIP # {clip}]") :
        rgb, mask_coarse, mask_refine = tools.vis_mask_pil(pil[img_id], foreground[img_id], args)
        masks.append(mask_refine)
        if args.visualize:
            rgb.save(os.path.join(tokencut_output_dir, img_names[img_id])) 

    del pil, feats, arr_h, arr_w, foreground
    return masks

def get_active_bboxes(args):
    cfg = setup_cfg(args)
    demo = VisualizationDemo(cfg)

    # 각 이미지의 detectron2 object detection에 대해 mask 값 확인
    for root, dirs, files in os.walk(args.data_path):
        
        if len(files) < 2: continue
        
        # output directoy 만들기
        video, clip = root.split('/')[-2:]
        if not video.isdigit(): continue
        try:
            active_people_bboxes = load_data(args)
        except:
            active_people_bboxes = {}

        if (int(video), int(clip)) not in active_people_bboxes: 
            active_people_bboxes[(int(video), int(clip))] = {}
        elif len(active_people_bboxes[(int(video), int(clip))].keys()) == 41: continue
        
        print(f"========== VIDEO # {video}, CLIP # {clip} ==========")

        # video clip에 대한 tokencut segmentation 결과 mask 배열에 저장 & 시각화 결과 저장
        masks = get_tokencut_mask(args, root)

        if args.visualize:
            detectron2_output_dir = os.path.join(args.output, "detectron2", video, clip)
            active_people_output_dir = os.path.join(args.output, "active_people_bboxes", video, clip)
            create_folder(detectron2_output_dir)
            create_folder(active_people_output_dir)

        for idx, image_name in enumerate(tqdm.tqdm(files, desc=f"Active Bboxes  [VIDEO # {video}, CLIP # {clip}]")):
            if not image_name.endswith(".jpg"): continue
            frame = int(image_name.split('.')[0])
            if frame in active_people_bboxes[(int(video), int(clip))]: continue

            image_path = os.path.join(root, image_name)
            img = read_image(image_path, format="BGR") # numpy.ndarray, (720, 1280, 3)

            predictions, detectron2_visualized_output = demo.run_on_image(img)

            if args.visualize: # VISUALIZE OBJECT DETECTION OF DETECTRON2
                detectron2_visualized_output.save(os.path.join(detectron2_output_dir, image_name))

            indexes = np.where(predictions['instances'].pred_classes.cpu().numpy() == 0) # THE INDEXES OF PEOPLE
            bboxes = predictions['instances'].pred_boxes[indexes] # THE BBOXES OF PEOPLE

            roi_bboxes = np.empty((0, 4), int) # A NUMPY ARRAY TO SAVE THE BBOXES OF ACTIVE PEOPLE
            img = np.ascontiguousarray(img, dtype=np.uint8)
            for bbox in bboxes:
                x1, y1, x2, y2 = map(int, bbox.cpu().numpy())
                if np.any(masks[idx][y1:y2, x1:x2] == 1): 
                    roi_bboxes = np.append(roi_bboxes, np.array([[y1, x1, y2, x2]]), axis=0)
                    if args.visualize: # DRAW THE BBOX OF THE ACTIVE PERSON
                        img = cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)
                        # cv2.imwrite(os.path.join(active_people_output_dir, f"{image_name.split('.')[0]}_{idx}.jpg"), img[y1:y2, x1:x2]) # SAVE THE CROPPED BBOX OF ACTIVE PERSON
            if args.visualize: # VISUALIZE THE IMAGE WITH THE BBOXES OF ACTIVE PEOPLE
                cv2.imwrite(os.path.join(active_people_output_dir, image_name), img)
            
            # SAVE THE ARRAY
            active_people_bboxes[(int(video), int(clip))][frame] = roi_bboxes
            save_data(args, active_people_bboxes)

if __name__ == "__main__":
    ErrorLog("Start!")

    mp.set_start_method("spawn", force=True)
    args = get_parser().parse_args()

    create_folder(args.output)
    try:
        get_active_bboxes(args)
    except:
        err = traceback.format_exc()
        ErrorLog(str(err))

    ErrorLog("The End")